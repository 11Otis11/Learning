# 概念

总的来说，**知识蒸馏**就是把知识交给**性能强，体量大**的大模型进行处理后，再将处理后的知识输入给**性能弱，体量小**的大模型的过程。

知识蒸馏的目的是：通过将被性能强效果好，但是费用高思考时间长的大模型A处理后的知识输入给另一个大模型B，让大模型B学习A的思维方式从而接近A的性能。这样能用一个强但是贵的大模型指导生成许多个简单但是便宜的大模型，从而做到用较低的成本批量生产性能较好的大模型。

# 举例剖析

首先了解两个概念：硬标签和软标签。

- 硬标签是指传统使用的人工标注的标签。例如，在图片分类任务中，对于三分类猫、狗、雪豹的任务，当某张图片上的是猫，人们会给它标注[1,0,0]，也就是硬标签。这为大模型提供了基础指导，为大模型指明了基本规则，但忽略了很多信息。
- 软标签是大模型输出的一个概率分布，例如对于一张内容是皮毛白色且分布着黑色斑点的猫，人工标注是[1,0,0]，而大模型的预测可能是[0.6,0.1,0.3]，虽然这个标签不完全确定图片内容，但它也隐含了更多信息：图片上的动物最像猫，其次像雪豹。

知识蒸馏的主要过程就是将教师模型输出的软标签再输入给学生模型，让学生模型学习它的思维方式。

# 温度(Temperature)

温度是控制模型输出的一个重要参数，从数学角度上描述是：



![image-20251209162143197](C:\Users\19043\AppData\Roaming\Typora\typora-user-images\image-20251209162143197.png)

$z_i $是模型的原始输出

可以看到T=1时，温度参数对输出无影响。T>1时，输出的各个值之间的差异会变小，输出更加平滑，暴露出更多复杂的和内在的信息。T趋于无限大时，输出近似于均匀。反之，输出的差异变大，模型更关注最基本的信息，最终类似于硬标签。

# 4. 蒸馏损失（Distillation Loss）

学生同时优化两部分目标：

$$L=α⋅L_{hard}+(1−α)⋅L_{soft}$$​

$$L_{hard}$$：学生 vs 真实标签（如交叉熵）

$L_{soft}$：学生 vs 教师软输出（如 KL 散度）

α：权重平衡

在调参时，较大的T适合任务复杂或者师生模型差距过大的场景，较小的α适合学生模型的训练初期